{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        # '''                           CS 637\n",
    "                                      Homework - 2\n",
    "                                   KUSHAL SHANKAR RAJ\n",
    "                         Deep Learning with Pytorch on CIFAR10 Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Design and Implementation:\n",
    "\n",
    "In this assignment we'll be using PyTorch to construct a convolutional neural network. \n",
    "\n",
    "Our focus is to train the CNN on the CIFAR-10 data set to be able to classify images from the CIFAR-10 testing set into the ten categories present in the data set.\n",
    "\n",
    "The CIFAR-10 data set is composed of 60,000 32x32 colour images, 6,000 images per class, so 10 categories in total. The training set is made up of 50,000 images, while the remaining 10,000 make up the testing set.\n",
    "\n",
    "The categories are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck.\n",
    "\n",
    "First step: Importing required libraries and extracting dataset\n",
    "\n",
    "Next: Defining a Convolutional Neural network model.\n",
    "\n",
    "The network has the following layout,\n",
    "\n",
    "Input > Conv (ReLU) > MaxPool > Conv (ReLU) > MaxPool > FC (ReLU) > FC (ReLU) > FC (SoftMax) > 10 outputs\n",
    "\n",
    "where:\n",
    "Conv is a convolutional layer, ReLU is the activation function, MaxPool is a pooling layer, FC is a fully connected layer and SoftMax is the activation function of the output layer.\n",
    "\n",
    "Next: Relu activation is used after convolution to scale features to greater than or equal to zero.\n",
    "\n",
    "Second layer: Batch Normalization and Pooling. Role in reducing dimensions of the image, removing the\n",
    "              irrelevant features and preserving the relevant.\n",
    "\n",
    "Here i'm using a two layered convolutional neural network(2-D conv). Input is flattened before passing it to\n",
    "the fully connected layer.\n",
    "\n",
    "Next: Fully connected layer. Finally learning the necessary features of an image for image classification.\n",
    "\n",
    "Finally: The loss is calculated and gradients are calculated along backward propagation.\n",
    "Weights are updated accordingly for every iteration.\n",
    "\n",
    "We are using Relu as activation function. \n",
    "Softmax at the output layer for classification and cross entropy as loss function.\n",
    "\n",
    "The program is generalized in a way that the number of parameters of the neural network \n",
    "can be customised according the user’s flexibility. \n",
    "\n",
    "Various parameters are:\n",
    "\n",
    "\tLearning_rate and epochs: The learning rate and the number of iterations for training of the\n",
    "                               test data can be passed as an argument.\n",
    "\n",
    "\tBatch_size: Required batch size to train the neural network to learn the parameters, in order to classify\n",
    "                the images into ten classes.\n",
    "                \n",
    "Plot of Loss and Accuracies ar given below.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing req. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.preprocessing as norm\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the neural net classifier.\n",
    "    '''\n",
    "    # Load the raw CIFAR-10 data\n",
    "    X_train, y_train, X_test, y_test = load(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    X_train = X_train.astype(np.float64)\n",
    "    X_val = X_val.astype(np.float64)\n",
    "    X_test = X_test.astype(np.float64)\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2)\n",
    "    X_val = X_val.transpose(0, 3, 1, 2)\n",
    "    X_test = X_test.transpose(0, 3, 1, 2)\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train)\n",
    "\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    X_train /= std\n",
    "    X_val /= std\n",
    "    X_test /= std\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'mean': mean_image, 'std': std\n",
    "    }\n",
    "\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    ''' load single batch of cifar '''\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding ='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "def load(ROOT):\n",
    "    ''' load all of cifar '''\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {}\n",
    "Dict = get_CIFAR10_data('cifar-10-batches-py/')\n",
    "\n",
    "val_data = Dict['X_val']\n",
    "val_labels = Dict['y_val']\n",
    "\n",
    "test_data = Dict['X_test']\n",
    "test_labels = Dict['y_test']\n",
    "\n",
    "training_data = Dict['X_train']\n",
    "training_labels = Dict['y_train']\n",
    "\n",
    "mean = Dict['mean']\n",
    "stand = Dict['std']\n",
    "\n",
    "train_x  = torch.from_numpy(training_data).float()\n",
    "train_y = training_labels.astype(int);\n",
    "train_y = torch.from_numpy(train_y).float()\n",
    "\n",
    "val_d  = torch.from_numpy(val_data).float()\n",
    "val_l = val_labels.astype(int);\n",
    "val_l = torch.from_numpy(val_l).float()\n",
    "\n",
    "test_x  = torch.from_numpy(test_data).float()\n",
    "test_y = test_labels.astype(int);\n",
    "test_y = torch.from_numpy(test_y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Training-Validation-Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CNN model definition...\n",
    "class Conv_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5)\n",
    "        self.nor1 = nn.BatchNorm2d(12)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=16, kernel_size=5)\n",
    "        self.nor2 = nn.BatchNorm2d(16)\n",
    "        self.fc1 = nn.Linear(in_features=(16 * 5 * 5), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=80)\n",
    "        self.fc3 = nn.Linear(in_features=80, out_features=10)\n",
    "\n",
    "        #Forward propag..  \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.nor1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.nor2(self.conv2(x))))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def training_loss(self,traincal):\n",
    "        plt.plot(traincal)\n",
    "        plt.xlabel(\"No: of Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.title('----Training Loss----')\n",
    "        plt.show()\n",
    "        \n",
    "    def train_acc(self,accurate):\n",
    "        plt.plot(accurate)\n",
    "        plt.xlabel(\"No: of Iterations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title('----Training Accuracy----')\n",
    "        plt.show()\n",
    "            \n",
    "    def validation_loss(self,validcal):\n",
    "        plt.plot(validcal)\n",
    "        plt.xlabel(\"No: of Iterations\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.title('----Validation Loss----')\n",
    "        plt.show()\n",
    "   \n",
    "    # MODEL TRAINING AND VALIDATION\n",
    "    \n",
    "    def Model_Training(self,train_x,train_y,val_d,val_l,test_x,test_y,Learning_rate,batch_size,epochs):\n",
    "        Learning_rate = Learning_rate\n",
    "        batch_size = batch_size\n",
    "        epochs = epochs\n",
    "        net = Conv_Net()\n",
    "        \n",
    "        #Training the network. \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(net.parameters(), lr=Learning_rate, momentum=0.9)\n",
    "        print('\\n\\n--------------Reports - Plot & Evaluation------------------\\n')\n",
    "        iter = int(len(train_x)/batch_size) \n",
    "\n",
    "        loss_traincal = []\n",
    "        loss_validcal = []\n",
    "        train_accuracy = []\n",
    "\n",
    "        net = net.float()\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            k = 0\n",
    "            training_loss = 0.0\n",
    "            validation_loss = 0.0\n",
    "            correct_count = 0\n",
    "            total_count = 0\n",
    "\n",
    "            for i in range(0, iter):\n",
    "\n",
    "                train_data = train_x[k: k+batch_size]\n",
    "                train_label = train_y[k: k+batch_size]\n",
    "\n",
    "                #print(i,\" : \",train_data.shape)\n",
    "                inputs = train_data\n",
    "                labels = train_label.long()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_count += labels.size(0)\n",
    "                correct_count += (predicted == labels).sum().item()\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                training_loss += loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                k += batch_size\n",
    "\n",
    "                if(i==(iter-1)):\n",
    "                    p = 0\n",
    "                    batch = 8\n",
    "                    iterater = int(len(val_d)/batch) \n",
    "\n",
    "                    #print('Validation epoch: ',epoch+1)\n",
    "                    for w in range(0, iterater):\n",
    "\n",
    "                        validation_d = val_d[p: p+batch]\n",
    "                        validation_l = val_l[p: p+batch]\n",
    "\n",
    "                        #print(i,\" : \",train_data.shape)\n",
    "                        input_s = validation_d\n",
    "                        label = validation_l\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        output = net(input_s)\n",
    "\n",
    "                        loss = criterion(output, label.long())\n",
    "\n",
    "                        validation_loss += loss\n",
    "\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                        p += batch\n",
    "\n",
    "                    loss_validcal.append(validation_loss)\n",
    "                    #print('End of validation')\n",
    "\n",
    "            loss_traincal.append(training_loss)\n",
    "             \n",
    "            accu = (100 * correct_count)/total_count\n",
    "            train_accuracy.append(accu)\n",
    "            v = math.ceil((accu*100)/100)\n",
    "            print('Training Accuracy of the network @ Epoch ',epoch+1,' := ',v,'%')\n",
    "            print('----------------------------------------------')\n",
    "        \n",
    "        self.training_loss(loss_traincal)\n",
    "        self.train_acc(train_accuracy)\n",
    "        self.validation_loss(loss_validcal)\n",
    "        \n",
    "        batchss = 16\n",
    "        iteration = int(len(test_x)/batchss) \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            t = 0\n",
    "            for j in range(0, iteration):\n",
    "                test = test_x[t: t+batchss]\n",
    "                test_lab1 = test_y[t: t+batchss]\n",
    "\n",
    "                test_lab = test_lab1.long()\n",
    "\n",
    "                out = net(test)\n",
    "                _, predicted = torch.max(out.data, 1)\n",
    "                total += test_lab.size(0)\n",
    "                correct += (predicted == test_lab).sum().item()\n",
    "                t += batchss\n",
    "        \n",
    "        print('\\n----------------------------------------------\\n')\n",
    "        print('Accuracy of the network on the 1000 test images: %d %%' % (100 * correct / total))\n",
    "        print('\\n----------------------------------------------\\n')\n",
    "        \n",
    "        bat_size = 16\n",
    "        it = int(len(test_x)/bat_size) \n",
    "        classes = ('plane', 'car', 'bird', 'cat',\n",
    "                   'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        with torch.no_grad():\n",
    "            q = 0\n",
    "            for j in range(0, it):\n",
    "                test = test_x[q: q+bat_size]\n",
    "                test_lab = test_y[q: q+bat_size]\n",
    "\n",
    "                test_lab = test_lab.long()\n",
    "\n",
    "                out_put = net(test)\n",
    "                _, predicted = torch.max(out_put, 1)\n",
    "                c = (predicted == test_lab).squeeze()\n",
    "                for i in range(4):\n",
    "                    label = test_lab[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "                q += bat_size\n",
    "\n",
    "        print('Accuracies for Given Classes........\\n')\n",
    "        for i in range(10):\n",
    "            print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------Cifar-10 Data Classification Model-------\n",
      "\n",
      "Enter Learning Rate... (Recommmended 0.001)   : \n"
     ]
    }
   ],
   "source": [
    "net = Conv_Net()\n",
    "\n",
    "print(\"\\n\\n------Cifar-10 Data Classification Model-------\\n\")\n",
    "print(\"Enter Learning Rate... (Recommmended 0.001)   : \") \n",
    "Learning_rate = float(input()) \n",
    "print(\"Enter Batch Size....    (Recommmended 32)     : \") \n",
    "Batch_Size = int(input()) \n",
    "print(\"Enter No:of Epochs....  (Recommmended 40)     : \") \n",
    "Iterations_epoch = int(input())\n",
    "\n",
    "net.Model_Training(train_x,train_y,val_d,val_l,test_x,test_y,Learning_rate,Batch_Size,Iterations_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
